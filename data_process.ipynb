{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08864259",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20b65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasan\\anaconda3\\envs\\pytorch-nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from collections import Counter\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c527c6",
   "metadata": {},
   "source": [
    "### Unimportant for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de33b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "eval_ent_dict = {\"O\": 0,\n",
    "                 \"Term\": 1,\n",
    "                 \"Definition\": 2,\n",
    "                 \"Alias-Term\": 3,\n",
    "                 \"Referential-Definition\": 4,\n",
    "                 \"Referential-Term\": 5,\n",
    "                 \"Qualifier\": 6}\n",
    "\n",
    "inv_eval_ent_dict = {0: \"O\",\n",
    "                     1: \"Term\",\n",
    "                     2: \"Definition\",\n",
    "                     3: \"Alias-Term\",\n",
    "                     4: \"Referential-Definition\",\n",
    "                     5: \"Referential-Term\",\n",
    "                     6: \"Qualifier\"}\n",
    "\n",
    "\n",
    "keep_def_prob = 1\n",
    "keep_O_prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_token(token, tokenizer, lang_model):\n",
    "    if \"roberta\" in lang_model:\n",
    "        return tokenizer.decode(token).replace(\" \", \"\")\n",
    "    elif \"scibert\" in lang_model:\n",
    "        token_word = tokenizer.decode(token).lower()\n",
    "        return token_word if token_word[:2] != \"##\" else token_word[2:]\n",
    "    elif \"xlnet\" in lang_model:\n",
    "        return tokenizer.decode(token)\n",
    "    elif \"albert-base\" in lang_model:\n",
    "        return tokenizer.decode(token)\n",
    "    elif \"bert\" in lang_model:\n",
    "        token_word = tokenizer.decode(token)\n",
    "        return token_word if token_word[:2] != \"##\" else token_word[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f6e77",
   "metadata": {},
   "source": [
    "## Preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0daea101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_weird_letters(word, lang_model):\n",
    "    replacements_chars = [(\"ö\", \"o\"), (\"é\", \"e\"), (\"ê\", \"e\"), (\"ü\", \"u\"),\n",
    "                          (\"ó\", \"o\"), (\"â\", \"a\"), (\"ä\", \"a\"), (\"à\", \"a\"),\n",
    "                          (\"ç\", \"c\"), (\"ï\", \"i\"), (\"ô\", \"o\"), (\"û\", \"u\"),\n",
    "                          (\"ÿ\", \"y\"), (\"á\", \"a\")]\n",
    "    \n",
    "    if \"scibert\" in lang_model:\n",
    "        word = word.lower()\n",
    "        \n",
    "        for init_char, repl_char in replacements_chars:\n",
    "            word = word.replace(init_char, repl_char)\n",
    "    elif \"albert\" in lang_model:\n",
    "        word = word.lower()\n",
    "    elif \"xlnet\" in lang_model:\n",
    "        for init_char, repl_char in replacements_chars:\n",
    "            word = word.replace(init_char, repl_char)\n",
    "            word = word.replace(init_char.upper(), repl_char.upper())\n",
    "        \n",
    "    return word\n",
    "\n",
    "\n",
    "def correct_punctuation(word, lang_model):\n",
    "    word = word.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"’\", \"'\").replace(\"‘\", \"'\") \\\n",
    "                               .replace(\",\", \",\").replace(\"⋅\", \"*\").replace(\"—\", \"-\").replace(\"`\", \"'\")\n",
    "                               # replace(\"…\", \"...\").replace(\"º\", \"\")\n",
    "    \n",
    "    if \"scibert\" in lang_model:\n",
    "        word = word.replace(\"º\", \"*\")\n",
    "    elif \"roberta\" in lang_model:\n",
    "        word = word.replace(\"º\", \"\")\n",
    "    elif \"xlnet\" in lang_model:\n",
    "        word = word.replace(\"…\", \"...\")\n",
    "                               \n",
    "    return word\n",
    "\n",
    "\n",
    "def remove_greek(word, lang_model, tokenizer):\n",
    "    greek_alphabet = 'αβγδεζηθικλμνξοπρςστυφχψω' + 'αβγδεζηθικλμνξοπρςστυφχψω'.upper() + \"∆\"\n",
    "    \n",
    "    for letter in greek_alphabet:\n",
    "        word = tokenizer.unk_token if letter in word else word\n",
    "        \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c0399",
   "metadata": {},
   "source": [
    "## Main processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path, data_kind=\"train\"):\n",
    "    \n",
    "    cols = [\"Sentence #\", \"Word\", \"Tag\"]\n",
    "    rows = []\n",
    "    sentence_counter = 1\n",
    "    \n",
    "    number_of_files = len(os.listdir(path))\n",
    "\n",
    "    print(\"Tracking Progress: Current File Number / Total Files\")\n",
    "    for file_ct, filename in enumerate(os.listdir(path)):\n",
    "        \n",
    "        print(file_ct+1, \"/\", number_of_files, end=\", \")\n",
    "        \n",
    "        with open(os.path.join(path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "            for line in file:\n",
    "                if line != \"\\n\":\n",
    "                    tokens = line.split()\n",
    "                    word, entity = tokens[0], tokens[4] \n",
    "\n",
    "                    rows.append({\"Sentence #\": sentence_counter, # if it is not the above than append to dataset\n",
    "                                 \"Word\": word,\n",
    "                                 \"Tag\": entity})\n",
    "                else:\n",
    "                    sentence_counter += 1\n",
    "            \n",
    "    # creating dataframe\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    # Writing dataframe to csv\n",
    "    df.to_csv(\"data/{kind}.csv\".format(kind=data_kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd4c9",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1221e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking Progress: Current File Number / Total Files\n",
      "1 / 67, 2 / 67, 3 / 67, 4 / 67, 5 / 67, 6 / 67, 7 / 67, 8 / 67, 9 / 67, 10 / 67, 11 / 67, 12 / 67, 13 / 67, 14 / 67, 15 / 67, 16 / 67, 17 / 67, 18 / 67, 19 / 67, 20 / 67, 21 / 67, 22 / 67, 23 / 67, 24 / 67, 25 / 67, 26 / 67, 27 / 67, 28 / 67, 29 / 67, 30 / 67, 31 / 67, 32 / 67, 33 / 67, 34 / 67, 35 / 67, 36 / 67, 37 / 67, 38 / 67, 39 / 67, 40 / 67, 41 / 67, 42 / 67, 43 / 67, 44 / 67, 45 / 67, 46 / 67, 47 / 67, 48 / 67, 49 / 67, 50 / 67, 51 / 67, 52 / 67, 53 / 67, 54 / 67, 55 / 67, 56 / 67, 57 / 67, 58 / 67, 59 / 67, 60 / 67, 61 / 67, 62 / 67, 63 / 67, 64 / 67, 65 / 67, 66 / 67, 67 / 67, "
     ]
    }
   ],
   "source": [
    "kind = \"test\" # train / validation / test\n",
    "\n",
    "if kind==\"train\":\n",
    "    path = \"C:/Users/Hasan/Desktop/Barcelona_Data/FIB_Courses/HLE/Project/DeftEval2020/code/deft_corpus/data/deft_files/train\"\n",
    "elif kind==\"validation\":\n",
    "    path = \"C:/Users/Hasan/Desktop/Barcelona_Data/FIB_Courses/HLE/Project/DeftEval2020/code/deft_corpus/data/deft_files/dev\"\n",
    "elif kind==\"test\":\n",
    "    path = \"C:/Users/Hasan/Desktop/Barcelona_Data/FIB_Courses/HLE/Project/DeftEval2020/code/deft_corpus/data/test_files/labeled/subtask_2\"\n",
    "else:\n",
    "    print(\"For the data kind to process, please specify one of train/validation/test\")\n",
    "    \n",
    "process_data(path, data_kind=kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611eab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830a830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
